{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, convert_to_messages\n",
    "from os.path import dirname, join\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, MessagesState, END\n",
    "from langgraph.types import Command\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import os\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal, Union\n",
    "from os.path import dirname, join\n",
    "from dotenv import load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "load_dotenv(\"/Users/mastorga/Documents/BTE-LLM/.env\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): #field to ask for OpenAI API key\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import BTEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLM-based agent\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4.1\")  # Change model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idCosineSimilarity(id1: str, id2: str) -> float:\n",
    "    def getIDSynonymEntities(ids: list[str], base_url: str = \"https://name-lookup.ci.transltr.io/synonyms\"):\n",
    "        try:\n",
    "            encoded_ids = [curie.replace(\":\", \"%3A\") for curie in ids]\n",
    "            query = \"&\".join(f\"preferred_curies={eid}\" for eid in encoded_ids)\n",
    "            url = f\"{base_url}?{query}\"\n",
    "\n",
    "            response = requests.get(url, headers={\"accept\": \"application/json\"})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            return {curie: data.get(curie, {}).get(\"names\", []) for curie in ids}\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying synonyms for {ids}: {e}\")\n",
    "            return {curie: [] for curie in ids}\n",
    "    \n",
    "    def getCosineSimilarity(str1: str, str2: str):\n",
    "        str1_list = word_tokenize(str1)\n",
    "        str2_list = word_tokenize(str2)\n",
    "\n",
    "        sw = stopwords.words(\"english\")\n",
    "        str1_set = {w for w in str1_list if w not in sw}\n",
    "        str2_set = {w for w in str2_list if w not in sw}\n",
    "\n",
    "        rvector = str1_set | str2_set\n",
    "        l1, l2 = [], []\n",
    "\n",
    "        for w in rvector:\n",
    "            l1.append(1 if w in str1_set else 0)\n",
    "            l2.append(1 if w in str2_set else 0)\n",
    "\n",
    "        if sum(l1) == 0 or sum(l2) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        c = sum(l1[i] * l2[i] for i in range(len(rvector)))\n",
    "        return c / float((sum(l1) * sum(l2)) ** 0.5)\n",
    "\n",
    "    # Expand IDs via node normalization\n",
    "    id1list = list(nodeNormalize(id1))  # returns set\n",
    "    id2list = list(nodeNormalize(id2))\n",
    "\n",
    "    if not id1list or not id2list:\n",
    "        print(\"ISSUE WITH ENTITY: \")\n",
    "        print(id1)\n",
    "        return 0.0\n",
    "\n",
    "    # Fetch synonyms\n",
    "    synonyms1 = getIDSynonymEntities(id1list)\n",
    "    synonyms2 = getIDSynonymEntities(id2list)\n",
    "\n",
    "    # Flatten synonyms into one string each\n",
    "    ent1_text = \" \".join(name for names in synonyms1.values() for name in names)\n",
    "    ent2_text = \" \".join(name for names in synonyms2.values() for name in names)\n",
    "\n",
    "    return getCosineSimilarity(ent1_text, ent2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodeNormalize(entity: str):\n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "        payload = {\n",
    "            \"curies\": [entity],\n",
    "            \"conflate\": True,\n",
    "            \"description\": True,\n",
    "            \"drug_chemical_conflate\": False\n",
    "        }\n",
    "    \n",
    "        response = requests.post(\n",
    "            \"https://nodenormalization-sri.renci.org/1.5/get_normalized_nodes\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "    \n",
    "        data = response.json()\n",
    "        entry = data.get(entity)  # access the entityâ€™s entry\n",
    "        ids = set()\n",
    "    \n",
    "        if not entry:\n",
    "            return []\n",
    "    \n",
    "        # Main id\n",
    "        if \"id\" in entry and \"identifier\" in entry[\"id\"]:\n",
    "            ids.add(entry[\"id\"][\"identifier\"])\n",
    "    \n",
    "        # Equivalent IDs\n",
    "        for equiv in entry.get(\"equivalent_identifiers\", []):\n",
    "            identifier = equiv.get(\"identifier\")\n",
    "            if identifier:\n",
    "                ids.add(identifier.strip(\"[]'\\\" \"))\n",
    "\n",
    "        if len(ids) < 1:\n",
    "            print(entity)\n",
    "        \n",
    "        return ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error normalizing {entity}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def nodeNormalizationTest(ent: str, gtruth: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if ent and gtruth share any normalized IDs.\n",
    "    \"\"\"\n",
    "    ent_ids = nodeNormalize(ent)       \n",
    "    gtruth_ids = nodeNormalize(gtruth)\n",
    "\n",
    "    return len(set(ent_ids) & set(gtruth_ids)) > 0  # intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_marked_entities(text: str) -> list[str]:\n",
    "    # capture everything after ** until newline or another asterisk\n",
    "    return [m.strip() for m in re.findall(r\"\\*\\*(.*?)\\*\\*\", text)]\n",
    "\n",
    "def sriNameResolver(ent: str, base_url = \"https://name-lookup.ci.transltr.io/lookup\", is_bp: bool = False, k = 50) -> list:    \n",
    "    candidates = []\n",
    "    autoComplete = True\n",
    "    \n",
    "    processed_ent = ent.replace(\" \", \"%20\")\n",
    "\n",
    "    final_url = base_url + \"?string=\" + processed_ent + \"&autocomplete=\" + str(autoComplete).lower() + \"&limit=\" + str(k)\n",
    "\n",
    "    if is_bp:\n",
    "        final_url = final_url + \"&only_prefixes=GO&biolink_type=BiologicalProcess\"\n",
    "        \n",
    "    response = requests.get(final_url, headers={\"accept\": \"application/json\"})\n",
    "\n",
    "    candidate_list = json.loads(response.content.decode(\"utf-8\"))\n",
    "\n",
    "    for item in candidate_list:\n",
    "        parsed = {\n",
    "            \"label\": item.get(\"label\", \"\"),\n",
    "            \"curie\": item.get(\"curie\", \"\"),\n",
    "            \"score\": item.get(\"score\", \"\")\n",
    "        }\n",
    "\n",
    "        candidates.append(parsed)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def selectIDbp(entity: str, candidates: list):\n",
    "    choices = [\"\"]\n",
    "\n",
    "    for entry in candidates:\n",
    "        curie = entry.get(\"curie\")\n",
    "        choices.append(curie)\n",
    "\n",
    "    class selectedID(TypedDict):\n",
    "        \"\"\"The most appropriate CUI/ID from the given candidates\"\"\"\n",
    "        selectedID: Literal[*choices]\n",
    "    \n",
    "    select_prompt = f\"\"\"You are a smart biomedical assistant that can understand the context and the intent behind a query. \n",
    "                Be careful when choosing IDs for entities that can refer to different concepts (for example, HIV can refer either to the virus or the disease; you MUST choose the most appropriate concept/definition based on the query). \n",
    "                Use the context and the intent behind the query to choose the most appropriate ID. \n",
    "                Select the one most appropriate ID/CUI for {entity} from the list below:\n",
    "                {candidates}\n",
    "                If none of the choices are appropriate, return \"\".\n",
    "                Otherwise, return only the ID/CUI.\n",
    "                \"\"\"\n",
    "\n",
    "    # LLM selects most appropriate ID from list\n",
    "    selectedID = llm.with_structured_output(selectedID).invoke(select_prompt)\n",
    "\n",
    "    bioID = str(selectedID[\"selectedID\"])\n",
    "\n",
    "    # Printing chosen ID + definition, if any\n",
    "    print(entity + \" - \" + bioID + '\\n')\n",
    "\n",
    "    return bioID\n",
    "\n",
    "def checkAnswer(result, groundTruth): \n",
    "    answers = extract_marked_entities(result)\n",
    "    for answer in answers:\n",
    "        ans_ID = selectIDbp(answer, sriNameResolver(answer, is_bp=False))\n",
    "        if nodeNormalizationTest(ans_ID, groundTruth): \n",
    "            return True\n",
    "     \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(\n",
    "    df,\n",
    "    question_col: str,\n",
    "    ans_col: str,\n",
    "    ans_id_col: str,\n",
    "    tools: dict[str, object],\n",
    "    check_answer_fn,\n",
    "    similarity_fn\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalizable test function.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame with entities and ground-truth IDs\n",
    "        question_col: column containing the question to be answered\n",
    "        ans_col: column containing ground truth (e.g. \"drug_name\")\n",
    "        ans_id_col: column containing ground truth IDs (e.g. \"drug\")\n",
    "        tools: dict mapping tool_name -> tool_object (must have .invoke())\n",
    "        check_answer_fn: function(result_id, groundTruth) -> \"Correct\"/\"Incorrect\"\n",
    "        similarity_fn: function(id1, id2) -> float\n",
    "\n",
    "    Returns:\n",
    "        dict with tally counts for each tool\n",
    "    \"\"\"\n",
    "    n = len(df[question_col])\n",
    "    tallies = {tool_name: 0 for tool_name in tools}\n",
    "\n",
    "    # Add results columns to df\n",
    "    for tool_name in tools:\n",
    "        df[f\"{ans_id_col}_{tool_name}\"] = \"\"               # raw results dict\n",
    "        df[f\"{ans_id_col}_{tool_name}_ans\"] = \"\"           # best chosen ID\n",
    "        df[f\"{ans_id_col}_{tool_name}_correct\"] = False    # correctness flag\n",
    "        df[f\"{ans_id_col}_{tool_name}_similarity\"] = -1.0  # cosine similarity\n",
    "\n",
    "    for index, question in enumerate(df[question_col]):\n",
    "        groundTruth = df.iloc[index][ans_id_col]\n",
    "        groundTruthName = df.iloc[index][ans_col]\n",
    "        print(f\"\\n{index+1}. Question: {question}\\nGround truth: {groundTruth} - {groundTruthName}\\n\")\n",
    "\n",
    "        # Store per-question results\n",
    "        question_results = {}\n",
    "\n",
    "        for tool_name, tool in tools.items():\n",
    "            best_ans = None\n",
    "            best_sim = -1.0\n",
    "            check = False\n",
    "\n",
    "            try:\n",
    "                # Run tool\n",
    "                results = tool(question)\n",
    "                df.at[index, f\"{ans_id_col}_{tool_name}\"] = results\n",
    "\n",
    "                answers = extract_marked_entities(results)\n",
    "\n",
    "                # Pick best candidate\n",
    "                for answer in answers:\n",
    "                    ans_ID = selectIDbp(answer, sriNameResolver(answer, is_bp=False))\n",
    "                    \n",
    "                    sim = similarity_fn(ans_ID, groundTruth)\n",
    "                    if sim > best_sim:\n",
    "                        best_ans, best_sim = ans_ID, sim\n",
    "                        check = check_answer_fn(best_ans, groundTruth)\n",
    "                    if sim == 1:\n",
    "                        check = True\n",
    "                        break\n",
    "\n",
    "                # Update DataFrame\n",
    "                df.at[index, f\"{ans_id_col}_{tool_name}_ans\"] = best_ans\n",
    "                df.at[index, f\"{ans_id_col}_{tool_name}_correct\"] = check\n",
    "                df.at[index, f\"{ans_id_col}_{tool_name}_similarity\"] = best_sim\n",
    "                if check == True:\n",
    "                    tallies[tool_name] += 1\n",
    "                    df.at[index, f\"{ans_id_col}_{tool_name}_correct\"] = True\n",
    "\n",
    "                print(f\"\\n{tool_name} answer: {best_ans}; cosine similarity = {best_sim:.3f}\\n\\n-----\")\n",
    "\n",
    "            except Exception as e:\n",
    "                best_ans = None\n",
    "                best_sim = -1.0\n",
    "                check = f\"Error: {e}\"\n",
    "\n",
    "            # Save to question summary\n",
    "            question_results[tool_name] = {\n",
    "                \"ans\": best_ans,\n",
    "                \"sim\": best_sim,\n",
    "                \"check\": check,\n",
    "                \"tally\": tallies[tool_name]\n",
    "            }\n",
    "\n",
    "        # === Print summary for this question ===\n",
    "        for tool_name, result in question_results.items():\n",
    "            print(f\"\"\"{tool_name} answer: {result['ans']} - {result['check']}\n",
    "    cosine similarity with gt: {result['sim']:.3f}\n",
    "    tally: {result['tally']} / {n}\\n\"\"\")\n",
    "\n",
    "        print(\"{\\n---------------------------\\n\")\n",
    "\n",
    "    return tallies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_only(question):\n",
    "    return llm.invoke(question).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "geneDB = pd.read_csv('/Users/mastorga/Documents/BTE-LLM/Prototype/data/DMDB_mechanistic_genes_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "geneSet = geneDB.sample(50)\n",
    "geneSet[\"pairID\"] = geneSet.index\n",
    "geneSet[\"gene_str\"] = geneSet[\"protein_gene_symbol\"].apply(lambda x: x.strip(\"\"\"['\"]\"\"\"))\n",
    "geneSet[\"protein_str\"] = geneSet[\"protein_name\"].apply(lambda x: x.strip(\"\"\"['\"]\"\"\"))\n",
    "geneSet[\"protein_ID\"] = geneSet[\"protein\"].apply(lambda x: x.strip(\"\"\"['\"]\"\"\").replace(\"UniProt:\", \"UniProtKB:\"))\n",
    "geneSet = geneSet.reset_index(drop=True)  \n",
    "\n",
    "geneSet['question'] = \"Which gene plays the most significant role in how the drug\" + geneSet['drug_name'] + \" treats or impacts the disease \" + geneSet['disease_name'] + \"? Enumerate 5 answers and do not include anything else in your response. Each of your answer entities MUST be tagged with ** at the start AND end of the phrase (**....**), otherwise it will not be assessed\"\n",
    "\n",
    "print(geneSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geneSet.to_excel('/Users/mastorga/Documents/BTE-LLM/Prototype/logs/drug <- diseasebp/questionset_50questions_mechanisticgenes_09.03.25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "    \"LLM_only\": LLM_only,\n",
    "    \"Agentic_BTE\": BTEx\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tallies = run_test(\n",
    "    df=geneSet,\n",
    "    question_col=\"question\",\n",
    "    ans_col=\"gene_str\",\n",
    "    ans_id_col=\"protein_ID\",\n",
    "    tools=tools,\n",
    "    check_answer_fn=checkAnswer,\n",
    "    similarity_fn=idCosineSimilarity\n",
    ")\n",
    "\n",
    "print(\"Final tallies for the questions:\", tallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses\n",
    "\n",
    "geneSet.to_excel('/Users/mastorga/Documents/BTE-LLM/Prototype/logs/drug <- diseasebp/50questions_mechanisticgenes_09.03.25.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
